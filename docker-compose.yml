# docker-compose.yml moved to /opt/backend root for unified project structure
#
# OpenAI/OpenAPI Configuration:
# - Edit .env file to add your OPENAI_API_KEY

services:
  redis:
    image: redis:7-alpine
    container_name: backend-redis
    ports:
      - "6379:6379"
    restart: always
    volumes:
      - ./storage/redis:/data
    command: ["redis-server", "--bind", "0.0.0.0", "--port", "6379", "--appendonly", "yes", "--save", "60", "1", "--loglevel", "notice", "--maxmemory", "256mb", "--maxmemory-policy", "allkeys-lru"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 5s
    networks:
      - backend-net

  chroma:
    image: chromadb/chroma:latest
    container_name: backend-chroma
    ports:
      - "8002:8000" # Host 8002 -> Container 8000 (chromadb)
    restart: always
    volumes:
      - ./storage/chroma:/chroma
      - ./storage/chroma/onnx_cache:/chroma/onnx_cache
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=False
    networks:
      - backend-net

  ollama:
    image: ollama/ollama:latest
    container_name: backend-ollama
    restart: unless-stopped
    environment:
      - OLLAMA_RUNNER=cpu
      - OLLAMA_LLM_LIBRARY=cpu
      - OLLAMA_DEBUG=false
      - OLLAMA_HOST=0.0.0.0
      - MODEL_DIR=/root/.ollama
    volumes:
      - ./storage/ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - backend-net

  llm_backend:
    build:
      context: .
      dockerfile: Dockerfile
    image: backend-llm_backend
    container_name: backend-llm-backend
    # Run as user llama (UID 1000) for Linux deployment
    user: "1000:1000"
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
    ports:
      - "9099:9099"
    restart: always
    depends_on:
      redis:
        condition: service_healthy
      chroma:
        condition: service_started
      ollama:
        condition: service_started
    environment:
      - CACHE_TTL=604800
      - SIMILARITY_THRESHOLD=0.92
      - LLM_TIMEOUT=30
      - API_TIMEOUT=30
      - WEB_SEARCH_TIMEOUT=10
      - CONNECTION_TIMEOUT=5
      - READ_TIMEOUT=25
      - WRITE_TIMEOUT=5
      - CONNECTION_POOL_SIZE=10
      - MAX_KEEPALIVE_CONNECTIONS=5
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000
      - USE_HTTP_CHROMA=true
      - DEFAULT_MODEL=llama3.2:3b
      - USE_OLLAMA=true
      - JWT_SECRET=change_this_in_production
      - JWT_ALGORITHM=HS256
      - API_KEY=f2b985dd-219f-45b1-a90e-170962cc7082
      - MAX_REQUESTS_PER_MINUTE=60
      - USER_AGENT=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36
      - WATCHDOG_STARTUP_DELAY=10
      # Ollama Configuration (Primary mode)
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_LOG_LEVEL=INFO
      # OpenAI/OpenAPI Integration (Fallback mode)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_API_BASE_URL=${OPENAI_API_BASE_URL:-https://api.openai.com/v1}
      - OPENAI_API_MODEL=${OPENAI_API_MODEL:-gpt-4}
      - OPENAI_API_TIMEOUT=${OPENAI_API_TIMEOUT:-60}
      - ENABLE_OPENAI_API=false
      - USE_OPENAI_ONLY=false
      # Performance optimizations
      - WATCHDOG_CHECK_INTERVAL=30
      - WATCHDOG_STABLE_INTERVAL=60
      - CHROMA_TELEMETRY=false
      - LOG_LEVEL=INFO
      - ENABLE_COLORED_LOGS=false
      - EMBEDDING_MODEL=intfloat/e5-small-v2
      - EMBEDDING_PROVIDER=huggingface
      - SENTENCE_TRANSFORMERS_HOME=/opt/internal_cache/sentence_transformers
      - HF_HOME=/opt/internal_cache/sentence_transformers
      # chromadb ONNX cache location
      - CHROMA_ONNX_CACHE_DIR=/opt/cache/chroma/onnx_models
    command: ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "9099", "--log-level", "debug"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9099/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s
    volumes:
      - ./storage/backend:/opt/backend/data
      - ./storage/chroma/onnx_cache:/opt/cache/chroma/onnx_models
      - ./main.py:/opt/backend/main.py
      - ./config.py:/opt/backend/config.py
      - ./models.py:/opt/backend/models.py
      - ./startup.py:/opt/backend/startup.py
      - ./database.py:/opt/backend/database.py
      - ./database_manager.py:/opt/backend/database_manager.py
      - ./error_handler.py:/opt/backend/error_handler.py
      - ./human_logging.py:/opt/backend/human_logging.py
      - ./cache_manager.py:/opt/backend/cache_manager.py
      - ./watchdog.py:/opt/backend/watchdog.py
      - ./rag.py:/opt/backend/rag.py
      - ./model_manager.py:/opt/backend/model_manager.py
      - ./storage_manager.py:/opt/backend/storage_manager.py
      - ./adaptive_learning.py:/opt/backend/adaptive_learning.py
      - ./enhanced_document_processing.py:/opt/backend/enhanced_document_processing.py
      - ./enhanced_integration.py:/opt/backend/enhanced_integration.py
      - ./enhanced_streaming.py:/opt/backend/enhanced_streaming.py
      - ./feedback_router.py:/opt/backend/feedback_router.py
      - ./services:/opt/backend/services
      - ./routes:/opt/backend/routes
      - ./handlers:/opt/backend/handlers
      - ./pipelines:/opt/backend/pipelines
      - ./utilities:/opt/backend/utilities
      - ./persona.json:/opt/backend/persona.json
      # For development only: mount the whole codebase for live reload (do not use in production)
      # - ./:/opt/backend
    networks:
      - backend-net

  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: backend-openwebui
    ports:
      - "3000:8080"
    restart: always
    environment:
      # Ollama Configuration - DISABLED (using OpenAI API only)
      # - OLLAMA_BASE_URL=http://ollama:11434
      # - OLLAMA_LOG_LEVEL=INFO
      - ENABLE_OLLAMA_API=false
      # Custom OpenAPI Backend Configuration (Your LLM Backend) - PRIMARY MODE
      - OPENAI_API_KEY=f2b985dd-219f-45b1-a90e-170962cc7082
      - OPENAI_API_BASE_URL=http://llm_backend:9099/v1
      - ENABLE_OPENAI_API=true
      - OPENAI_API_MODEL=gpt-4 # Frontend default; actual backend model is set via env (see .env)
      - OPENAI_API_TIMEOUT=180
      - OPENAI_API_MAX_TOKENS=4096
      - BACKEND_API_URL=http://llm_backend:9099
      - BACKEND_API_KEY=f2b985dd-219f-45b1-a90e-170962cc7082
      # OpenWebUI Settings
      - POSTHOG_DISABLED=true
      - GLOBAL_LOG_LEVEL=INFO
      - ENABLE_TAGS_GENERATION=false
      - ENABLE_TITLE_GENERATION=false
      # Authentication Settings
      - ENABLE_SIGNUP=true
      - DEFAULT_USER_ROLE=user
      # File Upload Settings
      - ENABLE_FILE_UPLOAD=true
      - FILE_SIZE_LIMIT=100 # MB, must match backend config
      # Pipeline Configuration
      - ENABLE_PIPELINES=true
      - PIPELINES_URLS=http://llm_backend:9099
    volumes:
      - ./storage/openwebui:/app/backend/data
    depends_on:
      llm_backend:
        condition: service_started
    networks:
      - backend-net

  watchtower:
    image: containrrr/watchtower:latest
    container_name: backend-watchtower
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_POLL_INTERVAL=300
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_INCLUDE_RESTARTING=true
    networks:
      - backend-net

networks:
  backend-net:
    driver: bridge
