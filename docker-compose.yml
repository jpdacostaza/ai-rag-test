# docker-compose.yml - Backend services with automatic memory function installation
#
# This configuration automatically mounts the memory function to OpenWebUI on startup

services:
  redis:
    image: redis:7-alpine
    container_name: backend-redis
    ports:
      - "6379:6379"
    restart: always
    volumes:
      - ./storage/redis:/data
    command: ["redis-server", "--bind", "0.0.0.0", "--port", "6379", "--appendonly", "yes", "--save", "60", "1", "--loglevel", "notice", "--maxmemory", "256mb", "--maxmemory-policy", "allkeys-lru"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 5s
    networks:
      - backend-net

  chroma:
    image: chromadb/chroma:latest
    container_name: backend-chroma
    ports:
      - "8002:8000"
    restart: always
    volumes:
      - ./storage/chroma:/chroma
      - ./storage/chroma/onnx_cache:/chroma/onnx_cache
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=False
    networks:
      - backend-net

  ollama:
    image: ollama/ollama:latest
    container_name: backend-ollama
    restart: unless-stopped
    environment:
      - OLLAMA_RUNNER=cpu
      - OLLAMA_LLM_LIBRARY=cpu
      - OLLAMA_DEBUG=false
      - OLLAMA_HOST=0.0.0.0
      - MODEL_DIR=/root/.ollama
    volumes:
      - ./storage/ollama:/root/.ollama
    ports:
      - "11434:11434"
    networks:
      - backend-net

  memory_api:
    build:
      context: .
      dockerfile: Dockerfile.memory
    image: backend-memory-api
    container_name: backend-memory-api
    ports:
      - "8003:8000"
    restart: always
    environment:
      - REDIS_URL=redis://redis:6379
      - CHROMA_URL=http://chroma:8000
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000
    volumes:
      - ./enhanced_memory_api.py:/app/main.py
      - ./storage/memory:/app/data
    depends_on:
      redis:
        condition: service_healthy
      chroma:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - backend-net

  llm_backend:
    build:
      context: .
      dockerfile: Dockerfile
    image: backend-llm_backend
    container_name: backend-llm-backend
    user: "1000:1000"
    labels:
      - "com.centurylinklabs.watchtower.enable=false"
    ports:
      - "8001:8001"
    restart: always
    depends_on:
      redis:
        condition: service_healthy
      chroma:
        condition: service_started
    environment:
      - REDIS_URL=redis://redis:6379
      - CHROMA_URL=http://chroma:8000
      - OLLAMA_URL=http://ollama:11434
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CHROMA_HOST=chroma
      - CHROMA_PORT=8000
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      - PYTHONPATH=/opt/backend
      - STORAGE_PATH=/opt/backend/storage
      - TEMP_PATH=/tmp
      - LOG_LEVEL=INFO
    volumes:
      - ./storage/backend:/opt/backend/data
      - ./storage/chroma/onnx_cache:/opt/cache/chroma/onnx_models
      - ./main.py:/opt/backend/main.py
      - ./config:/opt/backend/config
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - backend-net

  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: backend-openwebui
    ports:
      - "3000:8080"
    restart: always
    environment:
      - ENABLE_OLLAMA_API=false
      - OPENAI_API_KEY=f2b985dd-219f-45b1-a90e-170962cc7082
      - OPENAI_API_BASE_URL=http://llm_backend:8001/v1
      - ENABLE_OPENAI_API=true
      - OPENAI_API_MODEL=gpt-4
      - OPENAI_API_TIMEOUT=180
      - OPENAI_API_MAX_TOKENS=4096
      - BACKEND_API_URL=http://llm_backend:8001
      - BACKEND_API_KEY=f2b985dd-219f-45b1-a90e-170962cc7082
      - POSTHOG_DISABLED=true
      - GLOBAL_LOG_LEVEL=INFO
      - ENABLE_TAGS_GENERATION=false
      - ENABLE_TITLE_GENERATION=false
      - ENABLE_SIGNUP=true
      - DEFAULT_USER_ROLE=user
      - ENABLE_FILE_UPLOAD=true
      - FILE_SIZE_LIMIT=100
      - ENABLE_COMMUNITY_SHARING=false
      - WEBUI_AUTH=false
    volumes:
      - ./storage/openwebui:/app/backend/data
      - ./storage/openwebui/memory_function_working.py:/app/backend/data/functions/memory_function.py:ro
    depends_on:
      llm_backend:
        condition: service_healthy
      memory_api:
        condition: service_healthy
    networks:
      - backend-net

  watchtower:
    image: containrrr/watchtower:latest
    container_name: backend-watchtower
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_POLL_INTERVAL=300
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_INCLUDE_RESTARTING=true
    networks:
      - backend-net

networks:
  backend-net:
    driver: bridge
