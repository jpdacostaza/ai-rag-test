services:
  # Redis - Memory cache and short-term storage
  redis:
    image: redis:7-alpine
    container_name: backend-redis
    ports:
      - "6379:6379"
    restart: always
    volumes:
      - ./storage/redis:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - backend-net

  # ChromaDB - Vector database for embeddings
  chroma:
    image: chromadb/chroma:latest
    container_name: backend-chroma
    ports:
      - "8000:8000"
    restart: always
    volumes:
      - ./storage/chroma:/chroma/chroma
    networks:
      - backend-net

  # Ollama - Language model service
  ollama:
    image: ollama/ollama:latest
    container_name: backend-ollama
    ports:
      - "11434:11434"
    restart: always
    volumes:
      - ./storage/ollama:/root/.ollama
    environment:
      - OLLAMA_ORIGINS="*"
      - OLLAMA_HOST=0.0.0.0:11434
    healthcheck:
      test: ["CMD-SHELL", "pgrep ollama || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - backend-net

  # Integrated Memory API - Enhanced memory system with built-in auto-setup
  memory_api:
    build:
      context: .
      dockerfile: Dockerfile.memory
    container_name: backend-memory-api
    ports:
      - "8001:8000"
    restart: always
    environment:
      - REDIS_URL=redis://redis:6379
      - CHROMA_URL=http://chroma:8000
      - OLLAMA_API=http://ollama:11434
      - OPENWEBUI_API=http://openwebui:8080
    volumes:
      - ./storage/memory:/app/data
      - ./storage/openwebui:/tmp/openwebui:rw  # Shared database access
    depends_on:
      redis:
        condition: service_healthy
      chroma:
        condition: service_started
      # Remove ollama dependency for initial startup - memory_api will wait for it internally
    networks:
      - backend-net

  # OpenWebUI - Main web interface  
  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: backend-openwebui
    ports:
      - "3000:8080"
    restart: always
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URLS=http://memory_api:8000
      - OPENAI_API_KEYS=memory-api-key
      - ENABLE_FUNCTIONS=true
      - ENABLE_COMMUNITY_SHARING=false
      - WEBUI_AUTH=false
      - DEFAULT_MODELS=llama3.2:3b
    volumes:
      - ./storage/openwebui:/app/backend/data
    depends_on:
      - ollama
      - memory_api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health", "||", "exit", "1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - backend-net

  # Watchtower - Automatic container updates (optional)
  watchtower:
    image: containrrr/watchtower:latest
    container_name: backend-watchtower
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_POLL_INTERVAL=3600  # Check every hour
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_INCLUDE_RESTARTING=true
    networks:
      - backend-net
    profiles:
      - optional  # Disabled by default

networks:
  backend-net:
    driver: bridge
