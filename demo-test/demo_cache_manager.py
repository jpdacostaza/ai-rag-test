#!/usr/bin/env python3
import hashlib
import json
import os
import sys
import time
import traceback
from datetime import datetime
from typing import Optional, Dict, Any

import redis

from cache_manager import CacheManager

"""
Real-world Cache Manager Demo
Demonstrates cache functionality in realistic scenarios including:
- Chat session caching
- System prompt change handling
- Performance optimization
- Format validation
- Cache statistics and monitoring
"""

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class CacheManagerDemo:
    """Real-world demonstration of cache manager functionality."""

    def __init__(self):
        """Initialize demo environment."""
        self.redis_client: Optional[redis.Redis] = None
        self.cache_manager: Optional[CacheManager] = None
        self.demo_stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "cache_sets": 0,
            "json_rejections": 0,
            "system_prompt_changes": 0,
        }

    def setup_demo_environment(self) -> bool:
        """Setup Redis and cache manager for demo."""
        try:
            print("ğŸ”§ Setting up demo environment...")

            # Connect to Redis (use main DB for demo)
            self.redis_client = redis.Redis(
                host="localhost",
                port=6379,
                db=0,  # Use main database
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
            )

            # Test connection
            self.redis_client.ping()
            print("âœ… Redis connection successful")

            # Initialize CacheManager
            self.cache_manager = CacheManager(self.redis_client)
            print("âœ… CacheManager initialized")

            return True
        except Exception as e:
            print(f"âŒ Failed to setup demo environment: {e}")
            return False

    def generate_chat_key(self, user_id: str, conversation_context: str) -> str:
        """Generate a cache key for chat responses."""
        # Create hash of conversation context for caching
        context_hash = hashlib.sha256(conversation_context.encode()).hexdigest()[:16]
        return f"chat:{user_id}:{context_hash}"

    def simulate_llm_response(self, prompt: str, delay: float = 0.5) -> str:
        """Simulate LLM response generation with artificial delay."""
        time.sleep(delay)  # Simulate LLM processing time
        
        # Generate realistic responses based on prompt content
        if "python" in prompt.lower():
            return "Python is a high-level programming language known for its simplicity and readability. It's excellent for beginners and supports multiple programming paradigms including object-oriented, functional, and procedural programming."
        elif "machine learning" in prompt.lower():
            return "Machine Learning is a subset of artificial intelligence that enables computers to learn and improve from data without being explicitly programmed. Popular libraries include scikit-learn, TensorFlow, and PyTorch."
        elif "web development" in prompt.lower():
            return "Web development involves creating websites and web applications. Frontend technologies include HTML, CSS, and JavaScript, while backend technologies include Python (Django/Flask), Node.js, and various databases."
        else:
            return f"This is a helpful response about: {prompt[:50]}... The response provides detailed information and practical guidance."

    def demo_basic_caching(self):
        """Demonstrate basic caching functionality."""
        print("\nğŸ¯ Demo 1: Basic Chat Response Caching")
        print("-" * 50)

        if not self.cache_manager:
            print("âŒ Cache manager not initialized")
            return

        user_id = "demo_user_1"
        prompt = "What is Python programming?"
        cache_key = self.generate_chat_key(user_id, prompt)

        # First request - cache miss
        print(f"ğŸ‘¤ User: {prompt}")
        print("ğŸ” Checking cache...")

        start_time = time.time()
        cached_response = self.cache_manager.get_with_validation(cache_key)
        cache_time = time.time() - start_time

        if cached_response is None:
            print("âŒ Cache miss - generating new response")
            self.demo_stats["cache_misses"] += 1

            # Generate response (simulate LLM call)
            print("ğŸ§  Calling LLM...")
            start_time = time.time()
            response = self.simulate_llm_response(prompt)
            llm_time = time.time() - start_time

            # Cache the response
            cache_success = self.cache_manager.set_with_validation(cache_key, response, 600)
            if cache_success:
                print("ğŸ’¾ Response cached successfully")
                self.demo_stats["cache_sets"] += 1
            else:
                print("âš ï¸ Failed to cache response")

            print(f"ğŸ¤– Response: {response}")
            print(f"â±ï¸ Total time: {llm_time:.3f}s (LLM: {llm_time:.3f}s, Cache: {cache_time:.3f}s)")
        else:
            print("âœ… Cache hit!")
            self.demo_stats["cache_hits"] += 1
            response = cached_response
            print(f"ğŸ¤– Cached Response: {response}")
            print(f"â±ï¸ Total time: {cache_time:.3f}s (Cache only)")

        # Second request - should be cache hit
        print(f"\nğŸ‘¤ User asks same question again: {prompt}")
        print("ğŸ” Checking cache...")

        start_time = time.time()
        cached_response = self.cache_manager.get_with_validation(cache_key)
        cache_time = time.time() - start_time

        if cached_response:
            print("âœ… Cache hit! (much faster)")
            self.demo_stats["cache_hits"] += 1
            print(f"ğŸ¤– Cached Response: {cached_response}")
            print(f"â±ï¸ Total time: {cache_time:.3f}s (Cache only)")
        else:
            print("âŒ Unexpected cache miss")

    def demo_json_rejection(self):
        """Demonstrate JSON response rejection."""
        print("\nğŸ¯ Demo 2: JSON Response Format Rejection")
        print("-" * 50)

        if not self.cache_manager:
            print("âŒ Cache manager not initialized")
            return

        user_id = "demo_user_2"
        prompt = "Give me a JSON response"
        cache_key = self.generate_chat_key(user_id, prompt)

        # Simulate a JSON response that should be rejected
        json_responses = [
            '{"response": "This is JSON format"}',
            '{"answer": "JSON should not be cached", "reason": "format validation"}',
            '{"error": "This looks like an error response"}',
        ]

        for i, json_response in enumerate(json_responses, 1):
            print(f"\nğŸ“ Attempt {i}: Trying to cache JSON response")
            print(f"   Content: {json_response}")

            cache_success = self.cache_manager.set_with_validation(
                f"{cache_key}_{i}", json_response, 60
            )

            if cache_success:
                print("âŒ ERROR: JSON response was incorrectly cached!")
            else:
                print("âœ… JSON response correctly rejected")
                self.demo_stats["json_rejections"] += 1

    def demo_system_prompt_change_detection(self):
        """Demonstrate system prompt change detection."""
        print("\nğŸ¯ Demo 3: System Prompt Change Detection")
        print("-" * 50)

        if not self.cache_manager:
            print("âŒ Cache manager not initialized")
            return

        # Initial system prompt
        initial_prompt = "You are a helpful assistant that provides clear and concise answers."
        print(f"ğŸ“‹ Initial system prompt: {initial_prompt}")

        # Check for system prompt changes
        self.cache_manager.check_system_prompt_change(initial_prompt)
        print("âœ… System prompt stored")

        # Cache some responses with current system prompt
        for i in range(3):
            cache_key = f"demo:system_test:{i}"
            response = f"Response {i+1} with initial system prompt"
            success = self.cache_manager.set_with_validation(cache_key, response, 600)
            if success:
                print(f"ğŸ’¾ Cached response {i+1}")
                self.demo_stats["cache_sets"] += 1

        # Simulate system prompt change
        print("\nğŸ“ Changing system prompt...")
        if not self.redis_client:
            print("âŒ Redis client not available")
            return
            
        self.redis_client.set("user_preferences:user_a", "dark_mode=true")
        self.redis_client.set("session:abc123", "active")

        # Get initial stats
        stats_before = self.cache_manager.get_cache_stats()
        print(f"ğŸ“Š Stats before prompt change: {stats_before}")

        new_prompt = "You are a creative assistant that provides innovative and out-of-the-box solutions."
        print(f"ğŸ“‹ New system prompt: {new_prompt}")

        # Detect prompt change and clear cache
        self.cache_manager.check_system_prompt_change(new_prompt)
        self.demo_stats["system_prompt_changes"] += 1

        stats_after = self.cache_manager.get_cache_stats()
        print(f"ğŸ“Š Stats after prompt change: {stats_after}")

        # Try to retrieve previously cached responses (should be cache miss)
        for i in range(3):
            cache_key = f"demo:system_test:{i}"
            cached = self.cache_manager.get_with_validation(cache_key)
            if cached:
                print(f"âŒ ERROR: Old response {i+1} still in cache after prompt change!")
            else:
                print(f"âœ… Response {i+1} correctly cleared from cache")

        # Cache new responses with new system prompt
        for i in range(2):
            new_cache_key = f"demo:new_system_test:{i}"
            response = f"Response {i+1} with new creative system prompt - very innovative!"
            cached = self.cache_manager.get_with_validation(new_cache_key)
            if not cached:
                print(f"ğŸ’¾ Caching new response {i+1} with updated prompt")
                success = self.cache_manager.set_with_validation(new_cache_key, response, 600)
                if success:
                    self.demo_stats["cache_sets"] += 1

    def demo_performance_benchmark(self):
        """Demonstrate cache performance benefits."""
        print("\nğŸ¯ Demo 4: Performance Benchmark")
        print("-" * 50)

        if not self.cache_manager:
            print("âŒ Cache manager not initialized")
            return

        test_queries = [
            "What is artificial intelligence?",
            "How does machine learning work?",
            "Explain neural networks",
            "What is deep learning?",
            "How to get started with AI?",
        ]

        print("ğŸš€ Running performance benchmark...")
        print("   Testing cache hits vs LLM calls")

        total_cache_time = 0
        total_llm_time = 0
        cache_hit_count = 0
        llm_call_count = 0

        # Test each query twice - first will be LLM call, second should be cache hit
        for query in test_queries:
            cache_key = self.generate_chat_key("benchmark_user", query)

            # First call (LLM)
            start_time = time.time()
            cached_response = self.cache_manager.get_with_validation(cache_key)
            if not cached_response:
                response = self.simulate_llm_response(query, delay=0.8)  # Simulate slower LLM
                llm_time = time.time() - start_time
                total_llm_time += llm_time
                llm_call_count += 1

                self.cache_manager.set_with_validation(cache_key, response, 600)
                print(f"ğŸ§  LLM call for: {query[:30]}... (took {llm_time:.3f}s)")

            # Second call (cache hit)
            start_time = time.time()
            cached_response = self.cache_manager.get_with_validation(cache_key)
            if cached_response:
                cache_time = time.time() - start_time
                total_cache_time += cache_time
                cache_hit_count += 1
                print(f"âš¡ Cache hit for: {query[:30]}... (took {cache_time:.3f}s)")

        # Calculate and display performance metrics
        avg_llm_time = total_llm_time / llm_call_count if llm_call_count > 0 else 0
        avg_cache_time = total_cache_time / cache_hit_count if cache_hit_count > 0 else 0
        speedup = avg_llm_time / avg_cache_time if avg_cache_time > 0 else 0

        print(f"\nğŸ“Š Performance Results:")
        print(f"   ğŸ“ˆ Average LLM time: {avg_llm_time:.3f}s")
        print(f"   âš¡ Average cache time: {avg_cache_time:.3f}s")
        print(f"   ğŸš€ Speedup: {speedup:.1f}x faster with cache")

    def demo_cache_statistics(self):
        """Demonstrate cache statistics monitoring."""
        print("\nğŸ¯ Demo 5: Cache Statistics")
        print("-" * 50)

        if not self.cache_manager:
            print("âŒ Cache manager not initialized")
            return

        # Get current cache statistics
        stats = self.cache_manager.get_cache_stats()
        print("ğŸ“Š Current Cache Statistics:")
        for key, value in stats.items():
            print(f"   {key}: {value}")

        # Calculate demo-specific metrics
        total_requests = self.demo_stats["cache_hits"] + self.demo_stats["cache_misses"]
        if total_requests > 0:
            hit_rate = (self.demo_stats["cache_hits"] / total_requests) * 100
            print(f"\nğŸ¯ Demo Session Metrics:")
            print(f"   ğŸ“Š Total requests: {total_requests}")
            print(f"   âœ… Cache hits: {self.demo_stats['cache_hits']}")
            print(f"   âŒ Cache misses: {self.demo_stats['cache_misses']}")
            print(f"   ğŸ’¾ Cache sets: {self.demo_stats['cache_sets']}")
            print(f"   ğŸ“ˆ Hit rate: {hit_rate:.1f}%")

            if self.demo_stats["cache_sets"] > 0:
                efficiency_rate = (self.demo_stats["cache_hits"] / self.demo_stats["cache_sets"]) * 100
                print(f"   ğŸ¯ Cache efficiency: {efficiency_rate:.1f}%")

        print(f"   ğŸš« JSON rejections: {self.demo_stats['json_rejections']}")
        print(f"   ğŸ”„ System prompt changes: {self.demo_stats['system_prompt_changes']}")

    def run_full_demo(self):
        """Run complete cache manager demonstration."""
        print("ğŸ¬ Starting Cache Manager Demo")
        print("=" * 60)

        if not self.setup_demo_environment():
            print("âŒ Demo setup failed")
            return

        try:
            # Run all demo scenarios
            self.demo_basic_caching()
            self.demo_json_rejection()
            self.demo_system_prompt_change_detection()
            self.demo_performance_benchmark()
            self.demo_cache_statistics()

        except Exception as e:
            print(f"âŒ Demo error: {e}")
            traceback.print_exc()

        print("\n" + "=" * 60)
        print("ğŸ¬ Cache Manager Demo Complete")

    def generate_demo_report(self) -> Dict[str, Any]:
        """Generate comprehensive demo report."""
        if not self.cache_manager:
            return {"error": "Cache manager not initialized"}
            
        return {
            "demo_timestamp": datetime.now().isoformat(),
            "demo_statistics": self.demo_stats,
            "cache_statistics": self.cache_manager.get_cache_stats(),
            "environment": {
                "redis_connected": self.redis_client is not None,
                "cache_manager_ready": self.cache_manager is not None,
            },
            "performance_insights": {
                "total_operations": sum(self.demo_stats.values()),
                "cache_effectiveness": "High" if self.demo_stats["cache_hits"] > self.demo_stats["cache_misses"] else "Low"
            }
        }


def main():
    """Main demo execution."""
    demo = CacheManagerDemo()
    demo.run_full_demo()

    # Generate and save demo report
    report = demo.generate_demo_report()
    
    report_file = "cache_manager_demo_results.json"
    try:
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)
        print(f"\nğŸ“„ Demo report saved to: {report_file}")
    except Exception as e:
        print(f"âŒ Failed to save report: {e}")


if __name__ == "__main__":
    main()
